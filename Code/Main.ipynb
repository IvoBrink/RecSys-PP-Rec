{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:58:56.802485Z",
     "start_time": "2024-06-11T07:58:50.812875Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NewsContent import *\n",
    "from UserContent import *\n",
    "from preprocessing import *\n",
    "from PEGenerator import *\n",
    "import PEGenerator\n",
    "from models import *\n",
    "from utils import *\n",
    "from Encoders import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:58:58.257584Z",
     "start_time": "2024-06-11T07:58:58.059586Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root_path = \"../data/Challenge/\"\n",
    "embedding_path = \"../../\"\n",
    "KG_root_path = None\n",
    "popularity_path = \"../data/Challenge/popularity\"\n",
    "config = {'title_length':30,\n",
    "              'body_length':100,\n",
    "              'max_clicked_news':50,\n",
    "              'npratio':1,\n",
    "              'news_encoder_name':\"CNN\",\n",
    "              'user_encoder_name':\"Att\",\n",
    "             'attrs':['title','body','vert'],\n",
    "             'word_filter':0,\n",
    "             'data_root_path':data_root_path,\n",
    "             'embedding_path':embedding_path,\n",
    "             'KG_root_path':KG_root_path,\n",
    "            'popularity_path':popularity_path,\n",
    "            'batch_size': 32,\n",
    "             'max_entity_num':5}\n",
    "model_config = {\n",
    "        'news_encoder':0,\n",
    "        'popularity_user_modeling':True,\n",
    "        'rel':True,\n",
    "        'ctr':True,\n",
    "        'content':True,\n",
    "        'rece_emb':True,\n",
    "        'activity':True\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:35:17.505202Z",
     "start_time": "2024-06-11T08:35:16.705203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19779\n",
      "2472\n",
      "2473\n"
     ]
    }
   ],
   "source": [
    "News = NewsContent(config)\n",
    "TrainUsers = UserContent(News.news_index,config,'train.tsv',2)\n",
    "ValidUsers = UserContent(News.news_index,config,'val.tsv',1)\n",
    "TestUsers = UserContent(News.news_index,config,'test.tsv',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19779\n",
      "19916\n",
      "2473\n",
      "2472\n"
     ]
    }
   ],
   "source": [
    "train_sess,train_buckets, train_user_id, train_label = get_train_input(TrainUsers.session,News.news_index,config)\n",
    "test_impressions, test_userids = get_test_input(TestUsers.session,News.news_index)\n",
    "val_impressions, val_userids = get_test_input(ValidUsers.session,News.news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(News.news_stat_imp[train_sess[0, 0],train_buckets[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n"
     ]
    }
   ],
   "source": [
    "print(News.news_publish_bucket[train_sess[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,News.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbnerdTrainData(Dataset):\n",
    "    def __init__(self, News, User, session, user_id, buckets, labels):\n",
    "        self.News = News\n",
    "        self.User = User\n",
    "        self.session = session\n",
    "        self.buckets = buckets\n",
    "        self.user_id = user_id\n",
    "        self.labels = labels\n",
    "        self.length = len(self.session)\n",
    "        # ?\n",
    "        # self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        candidates = torch.IntTensor(self.session[idx])\n",
    "        candidates_data = torch.LongTensor(self.News.fetch_news(candidates))\n",
    "        candidates_ctr = torch.FloatTensor([self.News.get_ctr(x, self.buckets[idx]) for x in candidates])\n",
    "        candidates_rece_emb_index = torch.IntTensor([self.News.news_publish_bucket2[x] for x in candidates])\n",
    "        user_activity_input = 1\n",
    "        clicked_input = torch.IntTensor(self.User.click[self.user_id[idx]])\n",
    "        clicked_input_data = torch.LongTensor(self.News.fetch_news(clicked_input))\n",
    "        # print(clicked_input)\n",
    "        clicked_ctr = torch.FloatTensor([self.News.get_ctr(self.User.click[self.user_id[idx]][i], self.User.click_bucket[self.user_id[idx]][i]) for i in range(len(clicked_input))])\n",
    "        # print(clicked_ctr)\n",
    "        input = [candidates_data,candidates_ctr,candidates_rece_emb_index,user_activity_input,clicked_input_data,clicked_ctr]\n",
    "        # for x in range(len(input)):\n",
    "        #     input[x] = torch.FloatTensor(input[x])\n",
    "        labels = torch.FloatTensor(self.labels[idx])\n",
    "        return input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbnerdTestData(Dataset):\n",
    "    def __init__(self, News, User, user_id, impressions):\n",
    "        self.News = News\n",
    "        self.User = User\n",
    "        self.impressions = impressions\n",
    "        self.user_id = user_id\n",
    "        \n",
    "        self.length = len(self.user_id)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        candidates = self.impressions[idx]['docs']\n",
    "        candidates_ctr = [self.News.get_ctr(x, self.impressions[idx]['tsp']) for x in candidates]\n",
    "        candidates_rece_emb_index = [self.News.news_publish_bucket2[x] for x in candidates]\n",
    "        user_activity_input = 1\n",
    "        clicked_input = self.User.click[self.user_id[idx]]\n",
    "        # print(clicked_input)\n",
    "        clicked_ctr = [self.News.get_ctr(self.User.click[self.user_id[idx]][i], self.User.click_bucket[self.user_id[idx]][i]) for i in range(len(clicked_input))]\n",
    "        # print(clicked_ctr)\n",
    "        input = [candidates,candidates_ctr,candidates_rece_emb_index,user_activity_input,clicked_input,clicked_ctr]\n",
    "        for x in range(len(input)):\n",
    "            input[x] = torch.FloatTensor(input[x])\n",
    "        labels = self.impressions[idx]['labels']\n",
    "\n",
    "        return input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2473\n",
      "2488\n"
     ]
    }
   ],
   "source": [
    "# train_generator = TrainGenerator(News,TrainUsers,train_sess,train_user_id,train_buckets,train_label,32)\n",
    "# TODO for testing\n",
    "train_sess,train_buckets, train_user_id, train_label = get_train_input(TestUsers.session,News.news_index,config)\n",
    "\n",
    "train_loader = DataLoader(EbnerdTrainData(News, TrainUsers, train_sess, train_user_id, train_buckets, train_label), config['batch_size'])\n",
    "# # test_user_generator = UserGenerator(News,TestUsers,32)\n",
    "# test_loader = DataLoader(EbnerdTestData(News, TestUsers, test_userids, test_impressions), 32)\n",
    "# # val_user_generator = UserGenerator(News,ValidUsers,32)\n",
    "\n",
    "# val_loader = DataLoader(EbnerdTestData(News, ValidUsers, val_userids, val_impressions), 8)\n",
    "# # news_generator = NewsGenerator(News,32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'horoskoper': 0,\n",
       " 'auto': 1,\n",
       " 'haandvaerkeren': 2,\n",
       " 'dagsorden': 3,\n",
       " 'incoming': 4,\n",
       " 'penge': 5,\n",
       " 'om_ekstra_bladet': 6,\n",
       " 'biler': 7,\n",
       " 'nyheder': 8,\n",
       " 'sex_og_samliv': 9,\n",
       " 'podcast': 10,\n",
       " 'sport': 11,\n",
       " 'opinionen': 12,\n",
       " 'ferie': 13,\n",
       " 'krimi': 14,\n",
       " 'services': 15,\n",
       " 'nationen': 16,\n",
       " 'play': 17,\n",
       " 'bibliotek': 18,\n",
       " 'underholdning': 19,\n",
       " 'vin': 20,\n",
       " 'musik': 21,\n",
       " 'side9': 22,\n",
       " 'plus': 23,\n",
       " 'forbrug': 24}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "News.category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6559,   7914,    213,    138,  12011,    275,    374,     64,      6,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,    113,     67,    697,    213,     64,     54,\n",
      "           4369,   7914, 120597,   6360,     13, 120598, 120599,  23690,     11,\n",
      "           5208,   2314,   3548,     21,     76,   5525,    179, 120600,  21950,\n",
      "             36,    122,     30,   1628,    337,  18535,    225, 120601,     21,\n",
      "            300,    479,     98,    595,  17715,      8,   1885,    225,    113,\n",
      "         120602,     11,      9,   8381, 120603,     36,    453,     64,    138,\n",
      "           1204,  15188,    144,     23,     36,     13,    492,    509,    286,\n",
      "           1189,     51,     23,    735,   3671,     21,     13,     41,     98,\n",
      "          12042,  12011,     21,   7845, 120604,     98,  70138,     13,    184,\n",
      "             36,      9,     40,     15,   2918,     61,    845,     36,    122,\n",
      "             15,    227, 120597,     51,     23,    735,   3671,     21,    119,\n",
      "            120,     48,  53274,     21,      8],\n",
      "        [ 12697,     61, 126355,     82,     48,     30,     36,    189,   5480,\n",
      "             41,    848,      9,  10913,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,   3430,  14954,     13,   6050,    228,   2706,\n",
      "             21,     41,     98,     41,   6613,     36,     78,     15,   2673,\n",
      "          16600,     13,  20664,   1144,  82515,     29,    927, 109184,     48,\n",
      "           1962,     41,   1553,    144,   1739,     21,     24,     51,     97,\n",
      "             80,    122,   1928,  11204,     51,     76,  10881,  26712,     36,\n",
      "             13,  21984,     15,    241,   4288,    113,      9,    123,  11955,\n",
      "          11845,     21,     13,     41,   2154,     64,     20,     21,     41,\n",
      "            184,   1041,     13,  21188,   8602,  10726,   3523, 126356,     21,\n",
      "             41,  11251,     20,  13241,     36,     22,  21984,     98,     51,\n",
      "            248,     61,     41,     36,     82,   8058,     78,     76,  22478,\n",
      "          64550,     21,    122,     98,    595,     20,   2706,     13,  14954,\n",
      "             36,     22,    595,   1486,      8]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135960, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model,user_encoder,news_encoder,bias_news_encoder,bias_content_scorer,scaler,time_embedding_layer,activity_gater = \\\n",
    "create_pe_model(config, model_config, News, title_word_embedding_matrix, entity_embedding_matrix=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01282051282051282\n",
      "0.02564102564102564\n",
      "0.038461538461538464\n",
      "0.05128205128205128\n",
      "0.0641025641025641\n",
      "0.07692307692307693\n",
      "0.08974358974358974\n",
      "0.10256410256410256\n",
      "0.11538461538461539\n",
      "0.1282051282051282\n",
      "0.14102564102564102\n",
      "0.15384615384615385\n",
      "0.16666666666666666\n",
      "0.1794871794871795\n",
      "0.19230769230769232\n",
      "0.20512820512820512\n",
      "0.21794871794871795\n",
      "0.23076923076923078\n",
      "0.24358974358974358\n",
      "0.2564102564102564\n",
      "0.2692307692307692\n",
      "0.28205128205128205\n",
      "0.2948717948717949\n",
      "0.3076923076923077\n",
      "0.32051282051282054\n",
      "0.3333333333333333\n",
      "0.34615384615384615\n",
      "0.358974358974359\n",
      "0.3717948717948718\n",
      "0.38461538461538464\n",
      "0.3974358974358974\n",
      "0.41025641025641024\n",
      "0.4230769230769231\n",
      "0.4358974358974359\n",
      "0.44871794871794873\n",
      "0.46153846153846156\n",
      "0.47435897435897434\n",
      "0.48717948717948717\n",
      "0.5\n",
      "0.5128205128205128\n",
      "0.5256410256410257\n",
      "0.5384615384615384\n",
      "0.5512820512820513\n",
      "0.5641025641025641\n",
      "0.5769230769230769\n",
      "0.5897435897435898\n",
      "0.6025641025641025\n",
      "0.6153846153846154\n",
      "0.6282051282051282\n",
      "0.6410256410256411\n",
      "0.6538461538461539\n",
      "0.6666666666666666\n",
      "0.6794871794871795\n",
      "0.6923076923076923\n",
      "0.7051282051282052\n",
      "0.717948717948718\n",
      "0.7307692307692307\n",
      "0.7435897435897436\n",
      "0.7564102564102564\n",
      "0.7692307692307693\n",
      "0.782051282051282\n",
      "0.7948717948717948\n",
      "0.8076923076923077\n",
      "0.8205128205128205\n",
      "0.8333333333333334\n",
      "0.8461538461538461\n",
      "0.8589743589743589\n",
      "0.8717948717948718\n",
      "0.8846153846153846\n",
      "0.8974358974358975\n",
      "0.9102564102564102\n",
      "0.9230769230769231\n",
      "0.9358974358974359\n",
      "0.9487179487179487\n",
      "0.9615384615384616\n",
      "0.9743589743589743\n",
      "0.9871794871794872\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create your Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 3: Iterate over the data for the number of epochs\n",
    "for epoch in range(1):\n",
    "\n",
    "    # Step 4: Iterate over each batch of data and compute the scores using the forward pass of the network\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "    \n",
    "        # Step 5: Compute the lambda gradient values for the pairwise loss (spedup) with the compute_lambda_i method on the scores and the output labels\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        # Step 6: Bacward from the scores with the use of the lambda gradient values\n",
    "        if loss is not None:\n",
    "            # torch.autograd.backward(out, loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Step 7: Update the weights using the optimizer\n",
    "            optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "        print(i/train_loader.__len__())\n",
    "\n",
    "    # Step 8: At the end of the epoch, evaluate the model on the data using the evaluate_model function (both train and val)\n",
    "    model.eval()\n",
    "#     val_rankings = news_ranking(model_config,ctr_weight,val_predicted_activity_gates,val_user_scoring,news_scoring,\n",
    "#                                bias_candidate_score,news_bias_vecs,time_embedding_matrix,bias_content_scorer,\n",
    "#                                News,val_impressions)\n",
    "#     train_metrics = evaluate_model(data, model.forward, 'train')\n",
    "#     val_metrics = evaluate_model(data, model.forward, 'validation')\n",
    "    \n",
    "        \n",
    "#     print(f\"Epoch {epoch+1}/{params.epochs} - Train Metrics: {train_metrics}, Val Metrics: {val_metrics}\")\n",
    "\n",
    "#     # Step 9: Append the metrics to train_metrics_epoch and val_metrics_epoch\n",
    "#     train_metrics_epoch.append(train_metrics)\n",
    "#     val_metrics_epoch.append(val_metrics)\n",
    "    \n",
    "# return {\n",
    "#     \"metrics_train\": train_metrics_epoch,\n",
    "#     \"metrics_val\": val_metrics_epoch\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T07:59:00.558967Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.compile(loss=['categorical_crossentropy'],\n",
    "#                   optimizer=Adam(lr=0.0001), \n",
    "#                   metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T07:59:00.560967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    model,user_encoder,news_encoder,bias_news_encoder,bias_content_scorer,scaler,time_embedding_layer,activity_gater = create_pe_model(config,model_config,News,title_word_embedding_matrix,News.entity_embedding)\n",
    "    model.fit_generator(train_generator,epochs=2)\n",
    "    news_scoring = news_encoder.predict_generator(news_generator,verbose=True)\n",
    "    user_scoring = user_encoder.predict_generator(test_user_generator,verbose=True)\n",
    "    val_user_scoring = user_encoder.predict_generator(val_user_generator,verbose=True)\n",
    "\n",
    "\n",
    "    news_bias_vecs = bias_news_encoder.predict_generator(news_generator,verbose=True)\n",
    "\n",
    "    if model_config['content'] and not model_config['rece_emb']:\n",
    "        bias_candidate_score = bias_content_scorer.predict(news_bias_vecs,batch_size=32,verbose=True)\n",
    "        bias_candidate_score = bias_candidate_score[:,0]\n",
    "    else:\n",
    "        bias_candidate_score = 0\n",
    "\n",
    "    ctr_weight = scaler.get_weights()[0][0,0]\n",
    "    time_embedding_matrix = time_embedding_layer.get_weights()[0]\n",
    "    \n",
    "    predicted_activity_gates = activity_gater.predict(user_scoring,verbose=True)\n",
    "    predicted_activity_gates = predicted_activity_gates[:,0]\n",
    "    \n",
    "    val_predicted_activity_gates = activity_gater.predict(val_user_scoring,verbose=True)\n",
    "    val_predicted_activity_gates = val_predicted_activity_gates[:,0]\n",
    "    \n",
    "    rankings = news_ranking(model_config,ctr_weight,predicted_activity_gates,user_scoring,news_scoring,\n",
    "                                bias_candidate_score,news_bias_vecs,time_embedding_matrix,bias_content_scorer,\n",
    "                                News,test_impressions)\n",
    "    \n",
    "    val_rankings = news_ranking(model_config,ctr_weight,val_predicted_activity_gates,val_user_scoring,news_scoring,\n",
    "                               bias_candidate_score,news_bias_vecs,time_embedding_matrix,bias_content_scorer,\n",
    "                               News,val_impressions)\n",
    "    \n",
    "    performance = evaluate_performance(rankings,test_impressions)\n",
    "    val_performance = evaluate_performance(val_rankings,val_impressions)\n",
    "\n",
    "    cold = []\n",
    "    for TOP_COLD_NUM in [0,1,3,5,]:\n",
    "        g = evaluate_cold_users(rankings,test_impressions,TestUsers.click,TOP_COLD_NUM)\n",
    "        cold.append(g)\n",
    "    diversity = []\n",
    "    for TOP_DIVERSITY_NUM in range(1,11):\n",
    "        div_top = evaluate_diversity_topic_all(TOP_DIVERSITY_NUM,rankings,test_impressions,News,TestUsers)\n",
    "        div_ilxd = evaluate_density_ILxD(TOP_DIVERSITY_NUM,rankings,test_impressions,news_scoring)\n",
    "        diversity.append([div_top,div_ilxd])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
