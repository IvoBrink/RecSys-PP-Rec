{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:58:56.802485Z",
     "start_time": "2024-06-11T07:58:50.812875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NewsContent import *\n",
    "from UserContent import *\n",
    "from preprocessing import *\n",
    "from PEGenerator import *\n",
    "import PEGenerator\n",
    "from models import *\n",
    "from utils import *\n",
    "from Encoders import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:58:58.257584Z",
     "start_time": "2024-06-11T07:58:58.059586Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root_path = \"../data/Challenge/\"\n",
    "embedding_path = \"../../\"\n",
    "KG_root_path = \"../data/Challenge/entity\"\n",
    "popularity_path = \"../data/Challenge/popularity\"\n",
    "config = {'title_length':30,\n",
    "              'body_length':100,\n",
    "              'max_clicked_news':50,\n",
    "              'npratio':1,\n",
    "              'news_encoder_name':\"CNN\",\n",
    "              'user_encoder_name':\"Att\",\n",
    "             'attrs':['title', 'entity','vert'],\n",
    "             'word_filter':0,\n",
    "             'data_root_path':data_root_path,\n",
    "             'embedding_path':embedding_path,\n",
    "             'KG_root_path':KG_root_path,\n",
    "            'popularity_path':popularity_path,\n",
    "            'batch_size': 32,\n",
    "             'max_entity_num':5}\n",
    "model_config = {\n",
    "        'news_encoder':0,\n",
    "        'popularity_user_modeling':True,\n",
    "        'rel':True,\n",
    "        'ctr':True,\n",
    "        'content':True,\n",
    "        'rece_emb':True,\n",
    "        'activity':True\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:35:17.505202Z",
     "start_time": "2024-06-11T08:35:16.705203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19779\n",
      "2472\n",
      "2473\n"
     ]
    }
   ],
   "source": [
    "News = NewsContent(config)\n",
    "\n",
    "TrainUsers = UserContent(News.news_index,config,'train.tsv',2)\n",
    "ValidUsers = UserContent(News.news_index,config,'val.tsv',1)\n",
    "TestUsers = UserContent(News.news_index,config,'test.tsv',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19779\n",
      "19916\n",
      "2473\n",
      "2472\n"
     ]
    }
   ],
   "source": [
    "train_sess,train_buckets, train_user_id, train_label = get_train_input(TrainUsers.session,News.news_index,config)\n",
    "test_impressions, test_userids = get_test_input(TestUsers.session,News.news_index)\n",
    "val_impressions, val_userids = get_test_input(ValidUsers.session,News.news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,News.word_dict)\n",
    "entity_embedding_matrix, have_word2 = load_matrix(embedding_path, News.entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TrainDataset(News, TrainUsers, train_sess, train_user_id, train_buckets, train_label), config['batch_size'])\n",
    "test_user_data = UserDataset(News,TestUsers), config['batch_size']\n",
    "val_user_data = UserDataset(News,ValidUsers)\n",
    "news_data = NewsDataset(News)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(type(x[1]))\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model,user_encoder,news_encoder,bias_news_encoder,bias_content_scorer,scaler,time_embedding_layer,activity_gater = \\\n",
    "create_pe_model(config, model_config, News, title_word_embedding_matrix, entity_embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 623/623 [25:36<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "val_metrics_epoch = []\n",
    "num_epochs = 10\n",
    "# Step 2: Create your Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 3: Iterate over the data for the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "# Step 4: Iterate over each batch of data and compute the scores using the forward pass of the network\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "    \n",
    "        # Step 5: Compute the lambda gradient values for the pairwise loss (spedup) with the compute_lambda_i method on the scores and the output labels\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        # Step 6: Bacward from the scores with the use of the lambda gradient values\n",
    "        if loss is not None:\n",
    "            # torch.autograd.backward(out, loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Step 7: Update the weights using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_metrics = eval_model(model_config, News, user_encoder, val_impressions, val_user_data, val_userids,\n",
    "               news_encoder, bias_news_encoder, activity_gater, time_embedding_layer, \n",
    "               bias_content_scorer, scaler)\n",
    "    \n",
    "    print(\"epoch: {}, metrics: {}\".format(epoch, val_metrics))\n",
    "    val_metrics_epoch.append(val_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_model(model_config, News, user_encoder, test_impressions, test_user_data, test_userids,\n",
    "               news_encoder, bias_news_encoder, activity_gater, time_embedding_layer, \n",
    "               bias_content_scorer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2472/2472 [04:44<00:00,  8.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import trange\n",
    "# model.eval()\n",
    "# rankings = []\n",
    "# AUC = []\n",
    "# MRR = []\n",
    "# nDCG5 = []\n",
    "# nDCG10 =[]\n",
    "# for i in trange(len(val_impressions)):\n",
    "#     docids = val_impressions[i]['docs']\n",
    "#     docids = np.array(docids)\n",
    "#     bucket = val_impressions[i]['tsp']\n",
    "    \n",
    "#     publish_time = News.news_publish_bucket2[docids]\n",
    "\n",
    "#     if model_config['rel']:        \n",
    "#         uv = user_encoder(val_user_data.__getitem__(val_userids[i]))\n",
    "#         nv = news_encoder(torch.IntTensor(News.fetch_news(docids)))\n",
    "#         rel_score = torch.matmul(nv, uv[0])\n",
    "#         predicted_activity_gate = activity_gater(uv)\n",
    "#         predicted_activity_gate = predicted_activity_gate[:,0]\n",
    "\n",
    "#     else:\n",
    "#         rel_score = 0\n",
    "    \n",
    "#     if model_config['content'] and model_config['rece_emb']:\n",
    "#         bias_vecs = bias_news_encoder(torch.IntTensor(News.fetch_news(docids)))\n",
    "#         publish_time = bucket - publish_time\n",
    "#         arg = publish_time < 0\n",
    "#         publish_time[arg] = 0\n",
    "#         publish_bucket = compute_Q_publish(publish_time)\n",
    "#         time_emb = time_embedding_layer.weight[publish_bucket]\n",
    "#         bias_vecs = torch.cat([bias_vecs,time_emb], axis=-1)\n",
    "#         bias_score = bias_content_scorer(bias_vecs)\n",
    "#         bias_score = bias_score[:,0]\n",
    "#     else:\n",
    "#         bias_score = 0\n",
    "\n",
    "\n",
    "#     if model_config['activity']:\n",
    "#         gate = predicted_activity_gate\n",
    "#     else:\n",
    "#         gate = 0.5\n",
    "    \n",
    "#     if model_config['ctr']:\n",
    "#         ctr = torch.FloatTensor(fetch_ctr_dim1(News,docids,bucket,FLAG_CTR))\n",
    "#     else:\n",
    "#         ctr = 0\n",
    "\n",
    "#     score = gate*rel_score + (1-gate)*(ctr*scaler.scaler[0] + bias_score)\n",
    "#     score = score.detach().numpy()\n",
    "#     # print(score)\n",
    "\n",
    "#     labels = val_impressions[i]['labels']\n",
    "#     labels = np.array(labels)\n",
    "    \n",
    "#     auc = my_auc(labels,score)\n",
    "#     mrr = mrr_score(labels,score)\n",
    "#     ndcg5 = ndcg_score(labels,score,k=5)\n",
    "#     ndcg10 = ndcg_score(labels,score,k=10)\n",
    "\n",
    "#     # print(auc, mrr, ndcg5, ndcg10)\n",
    "\n",
    "#     AUC.append(auc)\n",
    "#     MRR.append(mrr)\n",
    "#     nDCG5.append(ndcg5)\n",
    "#     nDCG10.append(ndcg10)\n",
    "#     # return AUC, MRR, nDCG5, nDCG10\n",
    "\n",
    "#     # rankings.append(score)\n",
    "    \n",
    "\n",
    "# AUC = np.array(AUC)\n",
    "# MRR = np.array(MRR)\n",
    "# nDCG5 = np.array(nDCG5)\n",
    "# nDCG10 = np.array(nDCG10)\n",
    "\n",
    "# AUC = AUC.mean()\n",
    "# MRR = MRR.mean()\n",
    "# nDCG5 = nDCG5.mean()\n",
    "# nDCG10 = nDCG10.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold = []\n",
    "for TOP_COLD_NUM in [0,1,3,5,]:\n",
    "    g = evaluate_cold_users(rankings,test_impressions,TestUsers.click,TOP_COLD_NUM)\n",
    "    cold.append(g)\n",
    "diversity = []\n",
    "for TOP_DIVERSITY_NUM in range(1,11):\n",
    "    div_top = evaluate_diversity_topic_all(TOP_DIVERSITY_NUM,rankings,test_impressions,News,TestUsers)\n",
    "    div_ilxd = evaluate_density_ILxD(TOP_DIVERSITY_NUM,rankings,test_impressions,news_scoring)\n",
    "    diversity.append([div_top,div_ilxd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val metrics\", val_metrics_epoch)\n",
    "print(\"test metrics\", performance)\n",
    "print(\"cold\", cold)\n",
    "print(\"diversity\", diversity)\n",
    "\n",
    "results = {\"val metrics\": val_metrics_epoch, \n",
    "           \"test metrics\": performance,\n",
    "           \"cold\": cold,\n",
    "           \"diversity\": diversity}\n",
    "\n",
    "import json\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished\n",
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T07:59:00.558967Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.compile(loss=['categorical_crossentropy'],\n",
    "#                   optimizer=Adam(lr=0.0001), \n",
    "#                   metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T07:59:00.560967Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NewsContent' object has no attribute 'entity_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     model,user_encoder,news_encoder,bias_news_encoder,bias_content_scorer,scaler,time_embedding_layer,activity_gater \u001b[38;5;241m=\u001b[39m create_pe_model(config,model_config,News,title_word_embedding_matrix,\u001b[43mNews\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_embedding\u001b[49m)\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit_generator(train_generator,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m     news_scoring \u001b[38;5;241m=\u001b[39m news_encoder\u001b[38;5;241m.\u001b[39mpredict_generator(news_generator,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NewsContent' object has no attribute 'entity_embedding'"
     ]
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "\n",
    "#     model,user_encoder,news_encoder,bias_news_encoder,bias_content_scorer,scaler,time_embedding_layer,activity_gater = create_pe_model(config,model_config,News,title_word_embedding_matrix,News.entity_embedding)\n",
    "#     model.fit_generator(train_generator,epochs=2)\n",
    "#     news_scoring = news_encoder.predict_generator(news_generator,verbose=True)\n",
    "#     user_scoring = user_encoder.predict_generator(test_user_generator,verbose=True)\n",
    "#     val_user_scoring = user_encoder.predict_generator(val_user_generator,verbose=True)\n",
    "\n",
    "\n",
    "#     news_bias_vecs = bias_news_encoder.predict_generator(news_generator,verbose=True)\n",
    "\n",
    "#     if model_config['content'] and not model_config['rece_emb']:\n",
    "#         bias_candidate_score = bias_content_scorer.predict(news_bias_vecs,batch_size=32,verbose=True)\n",
    "#         bias_candidate_score = bias_candidate_score[:,0]\n",
    "#     else:\n",
    "#         bias_candidate_score = 0\n",
    "\n",
    "#     ctr_weight = scaler.get_weights()[0][0,0]\n",
    "#     time_embedding_matrix = time_embedding_layer.get_weights()[0]\n",
    "    \n",
    "#     predicted_activity_gates = activity_gater.predict(user_scoring,verbose=True)\n",
    "#     predicted_activity_gates = predicted_activity_gates[:,0]\n",
    "    \n",
    "#     val_predicted_activity_gates = activity_gater.predict(val_user_scoring,verbose=True)\n",
    "#     val_predicted_activity_gates = val_predicted_activity_gates[:,0]\n",
    "    \n",
    "#     rankings = news_ranking(model_config,ctr_weight,predicted_activity_gates,user_scoring,news_scoring,\n",
    "#                                 bias_candidate_score,news_bias_vecs,time_embedding_matrix,bias_content_scorer,\n",
    "#                                 News,test_impressions)\n",
    "    \n",
    "#     val_rankings = news_ranking(model_config,ctr_weight,val_predicted_activity_gates,val_user_scoring,news_scoring,\n",
    "#                                bias_candidate_score,news_bias_vecs,time_embedding_matrix,bias_content_scorer,\n",
    "#                                News,val_impressions)\n",
    "    \n",
    "#     performance = evaluate_performance(rankings,test_impressions)\n",
    "#     val_performance = evaluate_performance(val_rankings,val_impressions)\n",
    "\n",
    "#     cold = []\n",
    "#     for TOP_COLD_NUM in [0,1,3,5,]:\n",
    "#         g = evaluate_cold_users(rankings,test_impressions,TestUsers.click,TOP_COLD_NUM)\n",
    "#         cold.append(g)\n",
    "#     diversity = []\n",
    "#     for TOP_DIVERSITY_NUM in range(1,11):\n",
    "#         div_top = evaluate_diversity_topic_all(TOP_DIVERSITY_NUM,rankings,test_impressions,News,TestUsers)\n",
    "#         div_ilxd = evaluate_density_ILxD(TOP_DIVERSITY_NUM,rankings,test_impressions,news_scoring)\n",
    "#         diversity.append([div_top,div_ilxd])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
