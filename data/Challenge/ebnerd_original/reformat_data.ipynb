{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:27:59.554263Z",
     "start_time": "2024-06-21T11:27:58.333881Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install pandas pyarrow\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "articles = pd.read_parquet('articles.parquet', engine='pyarrow')\n",
    "behaviors = pd.read_parquet('train/behaviors.parquet', engine='pyarrow')\n",
    "history = pd.read_parquet('train/history.parquet', engine='pyarrow')\n",
    "\n",
    "# bert = pd.read_parquet('bert_base_multilingual_cased.parquet', engine='pyarrow')\n",
    "# display = bert['google-bert/bert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1f0a13b80f9a3d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T10:19:20.604094Z",
     "start_time": "2024-06-20T10:19:20.574064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                            ner_clusters  \\\n0                                                     []   \n1                                  [Harry, James Hewitt]   \n2                                                     []   \n3                                                     []   \n4                                                     []   \n...                                                  ...   \n11772  [Hawaiis, Kilauea, Kilauea, Reuters, US Geolog...   \n11773  [Alexandra Bøje, All England, Amalie Magelund,...   \n11774  [Cooper, Englemageren, Julie R, Julie R. Ølgaa...   \n11775  [Dnepr, Kherson, Nova Kakhovka, Nova Kakhovka,...   \n11776  [Adam, Bente, Birthe, Fyn, Fyns Politi, Hernin...   \n\n                                           entity_groups  \n0                                                     []  \n1                                             [PER, PER]  \n2                                                     []  \n3                                                     []  \n4                                                     []  \n...                                                  ...  \n11772                          [LOC, LOC, PER, ORG, ORG]  \n11773  [PER, PROD, PER, PER, MISC, MISC, PER, PER, LO...  \n11774              [PER, PROD, PER, PER, PER, PER, MISC]  \n11775     [LOC, LOC, LOC, PROD, PER, LOC, ORG, ORG, LOC]  \n11776  [PER, PER, PER, LOC, ORG, LOC, LOC, PER, PER, ...  \n\n[11777 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ner_clusters</th>\n      <th>entity_groups</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Harry, James Hewitt]</td>\n      <td>[PER, PER]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11772</th>\n      <td>[Hawaiis, Kilauea, Kilauea, Reuters, US Geolog...</td>\n      <td>[LOC, LOC, PER, ORG, ORG]</td>\n    </tr>\n    <tr>\n      <th>11773</th>\n      <td>[Alexandra Bøje, All England, Amalie Magelund,...</td>\n      <td>[PER, PROD, PER, PER, MISC, MISC, PER, PER, LO...</td>\n    </tr>\n    <tr>\n      <th>11774</th>\n      <td>[Cooper, Englemageren, Julie R, Julie R. Ølgaa...</td>\n      <td>[PER, PROD, PER, PER, PER, PER, MISC]</td>\n    </tr>\n    <tr>\n      <th>11775</th>\n      <td>[Dnepr, Kherson, Nova Kakhovka, Nova Kakhovka,...</td>\n      <td>[LOC, LOC, LOC, PROD, PER, LOC, ORG, ORG, LOC]</td>\n    </tr>\n    <tr>\n      <th>11776</th>\n      <td>[Adam, Bente, Birthe, Fyn, Fyns Politi, Hernin...</td>\n      <td>[PER, PER, PER, LOC, ORG, LOC, LOC, PER, PER, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11777 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[[\"ner_clusters\", \"entity_groups\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297da3de709bfcd",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "behaviors.sort_values('impression_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6db649de33b5b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test wikipedia entity embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a855bf3d6126a52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../../../glove.840B.300d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(lines[0]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:28:14.097044Z",
     "start_time": "2024-06-21T11:27:59.557253Z"
    }
   },
   "id": "98ab4e87855dc11f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities: 33078\n",
      "Percentage of articles without entities: 14.32453086524582%\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "nr_articles_no_entities = 0\n",
    "for ner_clusters in articles[\"ner_clusters\"]:\n",
    "    if len(ner_clusters) == 0:\n",
    "        nr_articles_no_entities += 1\n",
    "    for entity in ner_clusters:\n",
    "        entities.append(entity)\n",
    "unique_entities = set(entities)\n",
    "print(f\"Number of unique entities: {len(unique_entities)}\")\n",
    "print(f\"Percentage of articles without entities: {(nr_articles_no_entities / len(articles)) * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:28:15.893897Z",
     "start_time": "2024-06-21T11:28:14.118343Z"
    }
   },
   "id": "567ea5313cde23f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d750589f52df48a5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of entities found in the wikipedia embeddings: 20.39119656569321%\n"
     ]
    }
   ],
   "source": [
    "found = 0\n",
    "found_embeddings = []\n",
    "for line in lines[1:]:\n",
    "    entity = line.split()[0]\n",
    "    if entity in unique_entities:\n",
    "        found += 1\n",
    "        found_embeddings.append(entity)\n",
    "print(f\"Percentage of entities found in the wikipedia embeddings: {(found/len(unique_entities)  * 100)}%\")    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:29:36.156019Z",
     "start_time": "2024-06-21T11:28:56.970962Z"
    }
   },
   "id": "191cd77940a54536",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "6745"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T13:20:27.363653Z",
     "start_time": "2024-06-20T13:20:27.346184Z"
    }
   },
   "id": "e5e1cc814e54b0c3",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open(\"entities2id.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for idx,entity in enumerate(found_embeddings):\n",
    "        f.write(str(idx+1) +\"\\t\" + entity + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:31:38.782276Z",
     "start_time": "2024-06-21T11:31:38.760788Z"
    }
   },
   "id": "851af58ed0e77a64",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "33078"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T13:13:13.690452Z",
     "start_time": "2024-06-20T13:13:13.682449Z"
    }
   },
   "id": "4dc2f48bfe7e27e1",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T13:03:57.731879Z",
     "start_time": "2024-06-20T13:03:57.682702Z"
    }
   },
   "id": "554ffb82011bca2c",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "fb53536c2fc8d434",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create mergeimps.tsv (popularity)\n",
    "The \"mergeimps.tsv'' file needs to store the #click and #impression of each news with time information for building popularity signals. Specifically, we first need to divide the time into several buckets in units of one hour. Thus, since the MIND dataset contains user data in 6 weeks, the time periods can be divided into 2476 buckets. Then, we count the #impressions and #clicks of a news in each bucket and store the results in each line of ``mergeimps.tsv'', where the format is :\n",
    "\n",
    "[News_ID] TAB [Bucket_ID] TAB #Cliks TAB #Impressions.\n",
    " \n",
    "In MIND, since we did not obtain the click information for test data, which can substantially affect the estimation of popularity signals. Thus, I advise that you can use the validation dataset of MIND for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa24024ee9e3e1",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "behaviors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611fe96cb90f489",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# anchor = trans2tsp('05/08/2023 11:59:59 PM')\n",
    "# def parse_time_bucket(date):\n",
    "#     tsp = trans2tsp(date)\n",
    "#     tsp = tsp - anchor\n",
    "#     tsp = tsp//(600 * 2.2)\n",
    "#     return tsp\n",
    "\n",
    "\n",
    "# Assuming 'impression_time' is already in datetime format, if not convert it\n",
    "behaviors['impression_time'] = pd.to_datetime(behaviors['impression_time'])\n",
    "\n",
    "# Set the start date of your data collection period\n",
    "start_date = pd.to_datetime('2023-05-18')\n",
    "\n",
    "# Calculate the hour buckets since the start of the period\n",
    "behaviors['bucket_id'] = ((behaviors['impression_time'] - start_date).dt.total_seconds() // (600)).astype(int)\n",
    "\n",
    "# Check if 'article_ids_inview' is already in the desired format\n",
    "if not behaviors['article_ids_inview'].str.contains('N').any():\n",
    "    # For each line in the 'article_ids_inview' column, add 'N' upfront\n",
    "    # Add 'N' before each 'article_ids_inview', replace each space with ' N' and remove square brackets\n",
    "    behaviors['article_ids_inview'] = behaviors['article_ids_inview'].apply(lambda x: 'N' + str(x))\n",
    "    behaviors['article_ids_inview'] = behaviors['article_ids_inview'].str.replace(' ', ' N').str.replace('[', '').str.replace(']', '').str.replace('\\n', '')\n",
    "\n",
    "# Same for article_ids_clicked\n",
    "if not behaviors['article_ids_clicked'].str.contains('N').any():\n",
    "    behaviors['article_ids_clicked'] = behaviors['article_ids_clicked'].apply(lambda x: 'N' + str(x) )\n",
    "    behaviors['article_ids_clicked'] = behaviors['article_ids_clicked'].str.replace(' ', ' N').str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "behaviors[[\"bucket_id\", \"impression_time\", \"article_ids_inview\", \"article_ids_clicked\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc6279",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "behaviors[[\"bucket_id\", \"impression_time\", \"article_ids_inview\", \"article_ids_clicked\"]].sort_values('bucket_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172234fcab92af51",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "double_dict = {}\n",
    "\n",
    "for i, row in behaviors.iterrows():\n",
    "\n",
    "    for article_id in row[\"article_ids_inview\"].split(' '):\n",
    "        if str(article_id) + \"-\" + str(row[\"bucket_id\"]) not in double_dict:\n",
    "            double_dict[str(article_id) + \"-\" + str(row[\"bucket_id\"])] = {\"article_id\": article_id, \"bucket_id\": row[\"bucket_id\"], \"clicks\": 0, \"impressions\": 0}\n",
    "        double_dict[str(article_id) + \"-\" + str(row[\"bucket_id\"])][\"impressions\"] += 1\n",
    "            \n",
    "    for article_id in row[\"article_ids_clicked\"].split(' '):\n",
    "        if str(article_id) + \"-\" + str(row[\"bucket_id\"]) not in double_dict:\n",
    "            double_dict[str(article_id) + \"-\" + str(row[\"bucket_id\"])] = {\"article_id\": article_id, \"bucket_id\": row[\"bucket_id\"], \"clicks\": 0, \"impressions\": 0}\n",
    "        double_dict[str(article_id) + \"-\" + str(row[\"bucket_id\"])][\"clicks\"] += 1\n",
    "        \n",
    "mergeimps = pd.DataFrame(double_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651d10a9ed15b60",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# mergeimps.to_excel(\"mergeimps.xlsx\", index=False)\n",
    "mergeimps.to_csv(\"../popularity/mergeimps.tsv\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ed47add52961",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "test = mergeimps.groupby(\"article_id\").agg({\"clicks\": \"sum\", \"impressions\": \"sum\", \"bucket_id\" :[\"nunique\", \"count\"]}).reset_index()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc420c70a93f0f5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create V21UrlDocs22_title_json.tsv (necessary?)\n",
    "For implementing our method based on MIND, the \"V21UrlDocs22_title_json.tsv'' needs to store all news in the training/val/test set. Each line stores the information of a single news article, where format is based on \n",
    "\n",
    "[News_ID] TAB [Category] TAB [Subcategory] TAB [Title] TAB [Url] TAB [Abstract] TAB [Body].\n",
    " \n",
    "You can modify my codes to avoid the load of some unnecessary news data (e.g., Url, Abstract, and Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9badbe1552e75e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec16a8acea8728be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create train/val/test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28376853c0db5dea",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def process_ids(row):\n",
    "    clicked_ids = set(row['article_ids_clicked'].split())\n",
    "    inview_ids = row['article_ids_inview'].split()\n",
    "    processed_ids = [id for id in inview_ids if id not in clicked_ids]\n",
    "    return ' '.join(processed_ids)\n",
    "\n",
    "# Convert 'impression_time' to string\n",
    "behaviors['impression_time'] = behaviors['impression_time'].astype(str)\n",
    "\n",
    "# Check if 'impression_time' is already in the desired format\n",
    "if not behaviors['impression_time'].str.contains('/').any():\n",
    "    # Convert 'impression_time' to datetime\n",
    "    behaviors['impression_time'] = pd.to_datetime(behaviors['impression_time'])\n",
    "\n",
    "    # Format 'impression_time' in the desired format\n",
    "    behaviors['impression_time'] = behaviors['impression_time'].dt.strftime('%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Add 'U' before every 'user_id' if it doesn't already start with 'U'\n",
    "behaviors['user_id'] = behaviors['user_id'].apply(lambda x: 'U' + str(x) if not str(x).startswith('U') else str(x))\n",
    "\n",
    "# Convert 'article_ids_inview' to string\n",
    "behaviors['article_ids_inview'] = behaviors['article_ids_inview'].astype(str)\n",
    "\n",
    "# Check if 'article_ids_inview' is already in the desired format\n",
    "if not behaviors['article_ids_inview'].str.contains('N').any():\n",
    "    # For each line in the 'article_ids_inview' column, add 'N' upfront\n",
    "    # Add 'N' before each 'article_ids_inview', replace each space with ' N' and remove square brackets\n",
    "    behaviors['article_ids_inview'] = behaviors['article_ids_inview'].apply(lambda x: 'N' + str(x))\n",
    "    behaviors['article_ids_inview'] = behaviors['article_ids_inview'].str.replace(' ', ' N').str.replace('[', '').str.replace(']', '').str.replace('\\n', '')\n",
    "\n",
    "# Same for article_ids_clicked\n",
    "if not behaviors['article_ids_clicked'].str.contains('N').any():\n",
    "    behaviors['article_ids_clicked'] = behaviors['article_ids_clicked'].apply(lambda x: 'N' + str(x) )\n",
    "    behaviors['article_ids_clicked'] = behaviors['article_ids_clicked'].str.replace(' ', ' N').str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "# Select the required columns\n",
    "train_file = behaviors[['user_id', 'impression_time', 'article_ids_clicked', 'article_ids_inview']]\n",
    "\n",
    "# Write to a new .tsv file without index\n",
    "train_file.to_csv('../train.tsv', sep='\\t', index=False, header=True)\n",
    "\n",
    "# Read the file and print the head\n",
    "df = pd.read_csv('../train.tsv', sep='\\t')\n",
    "df['article_ids_inview'] = df['article_ids_inview'].str.replace('\"', '')\n",
    "df['article_ids_inview'] = df.apply(process_ids, axis=1)\n",
    "\n",
    "df['tot_clicks'] = df.apply(lambda row: (row['article_ids_clicked'], row['impression_time']),axis=1)\n",
    "df['positive'] = df['article_ids_clicked']\n",
    "df['negatives'] = df['article_ids_inview'].to_list()\n",
    "df['bucket'] = df['impression_time']\n",
    "\n",
    "df_grouped = df.groupby('user_id').agg({'tot_clicks': lambda x: x.tolist()}).reset_index()\n",
    "\n",
    "df = df.drop('tot_clicks', axis=1).merge(df_grouped[['user_id', 'tot_clicks']], on='user_id', how='left')\n",
    "\n",
    "docs = df[['user_id', 'tot_clicks', 'positive', 'negatives', 'bucket']]\n",
    "\n",
    "\n",
    "# Write the DataFrame back to the .tsv file\n",
    "train_split = docs[:int(len(docs)*0.8)]\n",
    "val_split = docs[int(len(docs)*0.8) : int(len(docs)*0.9)]\n",
    "test_split = docs[int(len(docs)*0.9) : ]\n",
    "\n",
    "train_split.to_csv('../train.tsv', sep='\\t', index=False, header=False)\n",
    "val_split.to_csv('../val.tsv', sep='\\t', index=False, header=False)\n",
    "test_split.to_csv('../test.tsv', sep='\\t', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908f36f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a5eb3",
   "metadata": {},
   "source": [
    "## Create docs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a105e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select the required columns\n",
    "articles['body'] = articles['body'].str.replace('\\n', ' ')\n",
    "articles['title'] = articles['title'].str.replace('\\n', ' ')\n",
    "articles['body'] = articles['body'].str.replace('\\t', ' ')\n",
    "# articles['body'] = articles['body'].fillna('')\n",
    "# articles['category_str'] = articles['category_str'].fillna('')\n",
    "articles['topics'] = articles['topics'].astype(str).str.replace('\\n', ' ')\n",
    "articles['article_id'] = articles['article_id'].apply(lambda x: 'N' + str(x) if not str(x).startswith('N') else str(x))\n",
    "docs_file = articles[['article_id', 'title', 'category_str', 'subcategory', 'body', 'topics']]\n",
    "\n",
    "# Write to a new .tsv file without index\n",
    "docs_file.to_csv('../docs.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f1180",
   "metadata": {},
   "source": [
    "## Create docs_pub_time.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9e4b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "articles['article_id'] = articles['article_id'].apply(lambda x: 'N' + str(x) if not str(x).startswith('N') else str(x))\n",
    "\n",
    "# Convert 'impression_time' to string\n",
    "articles['published_time'] = articles['published_time'].astype(str)\n",
    "\n",
    "# Check if 'impression_time' is already in the desired format\n",
    "if not articles['published_time'].str.contains('/').any():\n",
    "    # Convert 'impression_time' to datetime\n",
    "    articles['published_time'] = pd.to_datetime(articles['published_time'])\n",
    "\n",
    "    # Format 'impression_time' in the desired format\n",
    "    articles['published_time'] = articles['published_time'].dt.strftime('%m/%d/%Y %I:%M:%S %p')\n",
    "pubdocs_file = articles[['article_id', 'published_time']]\n",
    "\n",
    "# Write to a new .tsv file without index\n",
    "pubdocs_file.to_csv('../popularity/docs_pub_time.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28cfae8",
   "metadata": {},
   "source": [
    "## Create entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
